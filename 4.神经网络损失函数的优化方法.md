[哪里有高斯过程详细的数学推导？ - 舟晓南的回答 - 知乎](https://www.zhihu.com/question/352021284/answer/2479538915)

[高斯过程_1_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1V44y1z7yi?p=6&vd_source=fc8b0a4ba41cdbfcba300a85da1aea64)

##### 基本思想

1. $r\odot ,m \and(1-m)$
2. 标准化、归一化
3. 用衰减系数折中早期数据的影响
4. 梯度下降求极值点用于动量法，凸优化用于正则
5. 贝叶斯估计——高斯过程建模，来估计P(y|x，H),在已有高斯过程下计算未知点输出

##### 详述

1. 神经网络研究的函数高维，不能向低维的函数那样逃离局部极值点，要用随机梯度法逃离鞍点($\nabla f(x)=0,但H(x)非半正定的$)

   此外，神经网络陷入较坏的局部极值点概率较小，寻找局部极值点所在的平坦区域就能保证损失函数近似最小

2. 梯度的表示，设选取K个训练样本，$\vec g_t=\vec g_t(\theta_{t-1})=\frac 1 K \sum_{(x,y)\in S_t}\frac{\partial L(\vec y,f(\vec x;\theta))}{\partial \theta_{t-1}},\Delta\theta_t=-\alpha \vec g_t$

   为提高效率，采用小批量梯度下降，优化方法有

   1. 为了高效准确地收敛而调整学习率
      1. 指数下降、余弦函数下降

      2. 学习率预热，$\alpha_t'=\frac t {T'} \alpha_0$ 

      3. 周期式重启下降、

      4. 不同参数收敛性不同，$G_t=\sum_{i=1}^n g_i\odot g_i,\Delta\theta_t=-\frac \alpha {\sqrt {G_t+\epsilon}}\odot g_t$

      5. 自适应学习率算法，平滑下降，$G_t=\beta G_{t-1}+(1-\beta)g_t\odot g_t$

      6. 这次平滑的对象为$\Delta \theta_t$,$\Delta X_{t-1}^2=\beta_1 \Delta X_{t-2}^2+(1-\beta_1)\Delta\theta_{t-1}\odot\Delta\theta_{t-1}$

         $\Delta\theta_t=-{\sqrt{\Delta X_{t-1}^2+\epsilon}}\frac {g_t}{\sqrt{G_t+\epsilon}}$

   2. 梯度估计修正——随机梯度下降方法中每次迭代的梯度方向与整个训练集上的最优梯度并不一致，可以用最近一段时间的平均梯度代替当前时刻的随机梯度，提高优化速度

      1. momentum法:$\Delta \theta_t=\rho\Delta\theta_{t-1}-\alpha \vec{g_t}=-\alpha\sum_{i=1}^t\rho^{t-i}\vec{g_i}$

      * 可以看成两步，$\widehat{\theta}=\theta_{t-1}+\rho\Delta\theta_{t-1},\theta_t=\widehat{\theta}-\alpha\vec{g_t}$

      2. 第二步用$g_t(\widehat\theta)$代替$g_t(\theta_{t-1})$即得Nesterov加速梯度$\Delta \theta_t=\rho\Delta\theta_{t-1}-\alpha \vec{g_t}(\theta_{t-1}+\rho\Delta\theta_{t-1})$

      3. Adam法：利用$m,(1-m)$组合经验和当前梯度，用$G_t=\beta_2G_{t-1}+(1-\beta_2)g_t\odot g_t$来估计G_t

         $g_t本身用M_t估计，M_t=\beta_1M_{t-1}+(1-\beta_1)g_t$,再按照不同参数的收敛性改进$\theta$,$\Delta\theta_t=-\alpha\frac{\vec{M_t}}{\sqrt{\vec{G_t}+\epsilon}}$

      4. 梯度截断，按方向值或模

3. 参数的初始化方法有

   1. 预训练初始化、
   2. 随机初始化、
   3. 按照网络特征初始化为固定值

   随机初始化有3种主要方式，

   如初始化为$\mu=0,\sigma=k$的高斯分布和均匀分布，

   或者根据输入特征计算$\mu和\sigma$，再通过平移和缩放为$\mu=0,\sigma=k$

   specially,在等宽线性网络,$y=W^1W^2...W^kx$,因为$\delta^{(l-1)}=(W^{(l)})^T\delta^{(l)}$

   要使$||W^{(i)}||^2=1$,可以取均值=0，方差=1的高斯矩阵，再奇异值分解为正交矩阵$W^i$

4. 由于范数较大的特征在输入的影响较大，需要对输入特征进行初始化，

   可以按最大最小值、标准化处理，还可以用**白化**

5. 隐藏层的数据也要归一化，

   1. 对每种神经元内部的批量归一化，

   2. 对于同一层不同种神经元的层归一化，
   3. 对于权重，按照方向和权值的权重归一化
   4. 对卷积神经网络激活函数后输出的不同层的局部响应归一化

6. 神经网络在输入、隐藏层数据、权重、输出之外的超参数有

   1）网格数据：神经网络层数、各层神经元数、神经元的连接数、**激活函数的类型**

   2）优化参数：学习率、批量大小(batchsize)

   3）处理过拟合的正则化系数

   对应的优化方法：

   1）试探性的网格搜索（遍历超参数）随机搜索（随机选取超参数配置）

   2）利用试探与结果的**贝叶斯优化**、**动态资源分配**

   * 贝叶斯优化——从N个样本，高斯过程建模，计算cquiring function

     $EI(x,H)=\int max(y*-y)P(y|x,H)dy$，选出best x并入H，得到最优H*，用P(y|x,H\*)来估计P(y|x)

   * 动态资源分配，每次选取较好的样本

   3）利用控制器应用强化学习，以超参数配置的正确率为奖励，给出神经网络架构配置

7. 处理过拟合的正则化方法有

   1）l1和l2正则化,$l_1为了避免零点处导数不存在令l_1=\sum_{i=1}^n \sqrt {u_i^2+\epsilon}$

   2）**动量法权重衰减**

   3）当验证集的错误率不再减小时，停止迭代参数

   4）利用$\vec W \vec r\odot\vec X+\vec b$的丢弃法，测试时改成$\vec W p\vec x+\vec b$，相当于把训练时的输入取平均

   5）数据增强，主要在图像上，解决输入的数据集过小，如平移、放缩、旋转、增强噪音、

   6）标签平滑，解决交叉熵错误标签为0，梯度太大，改为$y=[\epsilon，\epsilon，...,1-(k-1)\epsilon,...,\epsilon]$,$\epsilon$可以为大数据在教师网络上的结果，作为指导，也叫知识蒸馏

