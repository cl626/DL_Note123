1. 具有记忆能力的3种网络的区别

   1. 延时神经网络=前向神经网络+前驱神经元历史信息
   2. 有外部输入的非线性自回归模型=外部历史输入+自身历史输出
   3. 循环神经网络=外部输入+前驱神经元历史(>>自身历史输出)

   * 均为处理序列式输入

2. 输出不仅与当前输入有关，还与之前的输入与输出有关

3. 通用近似定理拓展>>一个有足够数量的sigmoid神经元的循环神经网络可以模拟非线性动力系统

   $s_t=g(s_{t-1},x_t)\quad y_t=o(s_t)$，空间系统随时间的变换的函数表达

   上面这个神经网络是图灵-完备的，近似可以执行任何可计算问题

4. 应用场景、序列-标签、序列-序列(同步-只和当前输入、之前隐状态有关)、序列-序列(异步-编码器-译码器)

5. 参数优化、老实求导——**随时间反向传播算法**

    

6. 自回归网络的问题-梯度消失、梯度爆炸

7. 长短期记忆网络和门控神经网络像OS中的软件定义门

   $z_t,(1-z_t)$处理历史与当前，$r_t\odot$选择历史

8. 如何增强循环神经网络的表达能力

   * 增加深度>>1. 简单堆砌循环网络，
     2. 为处理后续时刻的信息，采用双向循环网络
   * 变换结构——1. 采用递归神经网络，特例=处理序列的循环神经网络
     2. 采用图结构——所有结点同步更新，$o_t=g(\{h_T^{(v)}|v\in V\})$,效率不如异步的循环/递归网络